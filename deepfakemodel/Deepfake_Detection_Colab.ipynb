{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bba4221",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: No GPU detected! Training will be very slow.\")\n",
    "    print(\"Go to Runtime → Change runtime type → GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0813bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q seaborn\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a617a",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive\n",
    "\n",
    "Upload your dataset to Google Drive first, then mount it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a231477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Change these paths to match your Google Drive structure\n",
    "DATASET_PATH = '/content/drive/MyDrive/FaceForensics++_C23'  # Update this path\n",
    "OUTPUT_PATH = '/content/frames_dataset'\n",
    "MODEL_SAVE_PATH = '/content/drive/MyDrive/deepfake_model'  # Save model to Drive\n",
    "\n",
    "!mkdir -p {OUTPUT_PATH}\n",
    "!mkdir -p {MODEL_SAVE_PATH}\n",
    "\n",
    "print(f\"✓ Google Drive mounted\")\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730caa1c",
   "metadata": {},
   "source": [
    "## 3. Video Preprocessing\n",
    "\n",
    "Extract frames from videos. This will take 1-2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c787603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class VideoPreprocessor:\n",
    "    def __init__(self, dataset_path, output_path, frames_per_video=10, target_size=(224, 224)):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.output_path = Path(output_path)\n",
    "        self.frames_per_video = frames_per_video\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Create output directories\n",
    "        (self.output_path / 'REAL').mkdir(parents=True, exist_ok=True)\n",
    "        (self.output_path / 'FAKE').mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def extract_frames(self, video_path, label):\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(str(video_path))\n",
    "            if not cap.isOpened():\n",
    "                return 0\n",
    "            \n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            if total_frames == 0:\n",
    "                cap.release()\n",
    "                return 0\n",
    "            \n",
    "            frame_indices = np.linspace(0, total_frames - 1, self.frames_per_video, dtype=int)\n",
    "            extracted_count = 0\n",
    "            video_name = Path(video_path).stem\n",
    "            \n",
    "            for frame_idx in frame_indices:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "                \n",
    "                resized_frame = cv2.resize(frame, self.target_size)\n",
    "                output_file = self.output_path / label / f\"{video_name}_frame_{extracted_count:03d}.jpg\"\n",
    "                cv2.imwrite(str(output_file), resized_frame)\n",
    "                extracted_count += 1\n",
    "            \n",
    "            cap.release()\n",
    "            return extracted_count\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {video_path}: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def process_fake_sequences(self):\n",
    "        fake_types = ['Deepfakes', 'Face2Face', 'FaceSwap', 'NeuralTextures']\n",
    "        total_extracted = 0\n",
    "        \n",
    "        for fake_type in fake_types:\n",
    "            video_dir = self.dataset_path / 'manipulated_sequences' / fake_type / 'c23' / 'videos'\n",
    "            if not video_dir.exists():\n",
    "                logger.warning(f\"Directory not found: {video_dir}\")\n",
    "                continue\n",
    "            \n",
    "            video_files = list(video_dir.glob('*.mp4'))\n",
    "            logger.info(f\"Processing {len(video_files)} {fake_type} videos\")\n",
    "            \n",
    "            for video_path in tqdm(video_files, desc=fake_type):\n",
    "                count = self.extract_frames(video_path, 'FAKE')\n",
    "                total_extracted += count\n",
    "        \n",
    "        return total_extracted\n",
    "    \n",
    "    def process_real_sequences(self):\n",
    "        video_dir = self.dataset_path / 'original_sequences' / 'youtube' / 'c23' / 'videos'\n",
    "        if not video_dir.exists():\n",
    "            logger.error(f\"Directory not found: {video_dir}\")\n",
    "            return 0\n",
    "        \n",
    "        video_files = list(video_dir.glob('*.mp4'))\n",
    "        logger.info(f\"Processing {len(video_files)} REAL videos\")\n",
    "        \n",
    "        total_extracted = 0\n",
    "        for video_path in tqdm(video_files, desc=\"Original sequences\"):\n",
    "            count = self.extract_frames(video_path, 'REAL')\n",
    "            total_extracted += count\n",
    "        \n",
    "        return total_extracted\n",
    "\n",
    "# Run preprocessing\n",
    "preprocessor = VideoPreprocessor(DATASET_PATH, OUTPUT_PATH, frames_per_video=10)\n",
    "print(\"Starting preprocessing...\")\n",
    "fake_count = preprocessor.process_fake_sequences()\n",
    "real_count = preprocessor.process_real_sequences()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Preprocessing completed!\")\n",
    "print(f\"FAKE frames: {fake_count}\")\n",
    "print(f\"REAL frames: {real_count}\")\n",
    "print(f\"Total frames: {fake_count + real_count}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c9bd89",
   "metadata": {},
   "source": [
    "## 4. Dataset Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count frames\n",
    "real_frames = len(list(Path(OUTPUT_PATH, 'REAL').glob('*.jpg')))\n",
    "fake_frames = len(list(Path(OUTPUT_PATH, 'FAKE').glob('*.jpg')))\n",
    "\n",
    "print(f\"REAL frames: {real_frames:,}\")\n",
    "print(f\"FAKE frames: {fake_frames:,}\")\n",
    "print(f\"Total: {real_frames + fake_frames:,}\")\n",
    "print(f\"Class ratio (FAKE/REAL): {fake_frames/real_frames:.2f}:1\")\n",
    "\n",
    "# Visualize distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(['REAL', 'FAKE'], [real_frames, fake_frames], color=['green', 'red'])\n",
    "plt.ylabel('Number of Frames')\n",
    "plt.title('Class Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie([real_frames, fake_frames], labels=['REAL', 'FAKE'], autopct='%1.1f%%', colors=['green', 'red'])\n",
    "plt.title('Class Percentage')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd5ddfe",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Train the deepfake detection model using ResNet50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e9caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class FaceForensicsDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load REAL images (label 0)\n",
    "        real_dir = self.data_path / 'REAL'\n",
    "        if real_dir.exists():\n",
    "            real_images = list(real_dir.glob('*.jpg'))\n",
    "            self.images.extend(real_images)\n",
    "            self.labels.extend([0] * len(real_images))\n",
    "        \n",
    "        # Load FAKE images (label 1)\n",
    "        fake_dir = self.data_path / 'FAKE'\n",
    "        if fake_dir.exists():\n",
    "            fake_images = list(fake_dir.glob('*.jpg'))\n",
    "            self.images.extend(fake_images)\n",
    "            self.labels.extend([1] * len(fake_images))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "class DeepfakeDetector(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(DeepfakeDetector, self).__init__()\n",
    "        self.backbone = models.resnet50(pretrained=pretrained)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "print(\"✓ Model classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = FaceForensicsDataset(OUTPUT_PATH, transform=train_transform)\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "\n",
    "# Split into train and validation\n",
    "val_split = 0.2\n",
    "val_size = int(len(dataset) * val_split)\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Apply different transforms to validation\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "print(f\"Train set: {train_size:,}\")\n",
    "print(f\"Val set: {val_size:,}\")\n",
    "\n",
    "# Data loaders\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"✓ Data loaders created (batch size: {BATCH_SIZE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cef8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = DeepfakeDetector(pretrained=True).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc='Training'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Validating'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader), correct / total, all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 20\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"\\nStarting training for {EPOCHS} epochs...\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\\n\")\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        }, f'{MODEL_SAVE_PATH}/best_model.pt')\n",
    "        print(f\"✓ Model saved (Val Loss: {val_loss:.4f})\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training completed!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604fe83a",
   "metadata": {},
   "source": [
    "## 6. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528841c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
    "ax2.plot(history['val_acc'], label='Val Accuracy', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_SAVE_PATH}/training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Training history saved to {MODEL_SAVE_PATH}/training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcbe4eb",
   "metadata": {},
   "source": [
    "## 7. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61528022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(f'{MODEL_SAVE_PATH}/best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"✓ Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "\n",
    "# Final evaluation\n",
    "val_loss, val_acc, all_preds, all_labels = validate(model, val_loader, criterion, device)\n",
    "\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Negatives (REAL as REAL): {cm[0,0]}\")\n",
    "print(f\"  False Positives (REAL as FAKE): {cm[0,1]}\")\n",
    "print(f\"  False Negatives (FAKE as REAL): {cm[1,0]}\")\n",
    "print(f\"  True Positives (FAKE as FAKE): {cm[1,1]}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['REAL', 'FAKE'], \n",
    "            yticklabels=['REAL', 'FAKE'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig(f'{MODEL_SAVE_PATH}/confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28abc17f",
   "metadata": {},
   "source": [
    "## 8. Inference on Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d78270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image_path, transform, device):\n",
    "    \"\"\"Predict on a single image\"\"\"\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        prediction = output.argmax(dim=1).item()\n",
    "    \n",
    "    result = {\n",
    "        'prediction': 'REAL' if prediction == 0 else 'FAKE',\n",
    "        'confidence': float(probabilities[0, prediction].item()),\n",
    "        'real_prob': float(probabilities[0, 0].item()),\n",
    "        'fake_prob': float(probabilities[0, 1].item())\n",
    "    }\n",
    "    \n",
    "    return result, image\n",
    "\n",
    "# Test on a few validation samples\n",
    "test_indices = [0, 100, 200, 300, 400]\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for i, idx in enumerate(test_indices):\n",
    "    if idx < len(val_dataset):\n",
    "        img_path = val_dataset.dataset.images[val_dataset.indices[idx]]\n",
    "        true_label = val_dataset.dataset.labels[val_dataset.indices[idx]]\n",
    "        \n",
    "        result, image = predict_image(model, img_path, val_transform, device)\n",
    "        \n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(\n",
    "            f\"Pred: {result['prediction']}\\n\"\n",
    "            f\"True: {'REAL' if true_label == 0 else 'FAKE'}\\n\"\n",
    "            f\"Conf: {result['confidence']:.2%}\",\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_SAVE_PATH}/sample_predictions.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Sample predictions saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c302bd8a",
   "metadata": {},
   "source": [
    "## 9. Download Results\n",
    "\n",
    "All results are automatically saved to your Google Drive at the path specified in `MODEL_SAVE_PATH`.\n",
    "\n",
    "**Saved files:**\n",
    "- `best_model.pt` - Trained model checkpoint\n",
    "- `training_history.png` - Training curves\n",
    "- `confusion_matrix.png` - Evaluation metrics\n",
    "- `sample_predictions.png` - Sample predictions\n",
    "\n",
    "You can also download the model directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a43895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download model (optional, already saved to Drive)\n",
    "# files.download(f'{MODEL_SAVE_PATH}/best_model.pt')\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"ALL RESULTS SAVED TO GOOGLE DRIVE\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Location: {MODEL_SAVE_PATH}\")\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  - best_model.pt\")\n",
    "print(f\"  - training_history.png\")\n",
    "print(f\"  - confusion_matrix.png\")\n",
    "print(f\"  - sample_predictions.png\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c99eec8",
   "metadata": {},
   "source": [
    "## 10. Using the Model Locally\n",
    "\n",
    "To use this model on your local machine:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load model\n",
    "model = DeepfakeDetector(pretrained=False)\n",
    "checkpoint = torch.load('best_model.pt', map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Prepare image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Predict\n",
    "image = Image.open('test.jpg').convert('RGB')\n",
    "input_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "    prediction = output.argmax(dim=1).item()\n",
    "\n",
    "print(f\"Prediction: {'REAL' if prediction == 0 else 'FAKE'}\")\n",
    "print(f\"Confidence: {probabilities[0, prediction].item():.2%}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f964069a",
   "metadata": {},
   "source": [
    "## Tips for Better Results\n",
    "\n",
    "1. **Use GPU Runtime**: Runtime → Change runtime type → GPU (T4)\n",
    "2. **Monitor Training**: Watch for overfitting (train acc >> val acc)\n",
    "3. **Adjust Learning Rate**: If loss plateaus early, reduce LR to 0.0001\n",
    "4. **Increase Epochs**: For better accuracy, train for 30+ epochs\n",
    "5. **Save Checkpoints**: Model is auto-saved to Google Drive\n",
    "6. **Check GPU Memory**: Reduce batch size if OOM errors occur\n",
    "7. **Class Imbalance**: Dataset has 4x more FAKE than REAL - this is expected\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **Out of Memory**: Reduce BATCH_SIZE to 16 or 8\n",
    "- **Slow Training**: Make sure GPU is enabled\n",
    "- **Low Accuracy**: Train for more epochs or adjust learning rate\n",
    "- **Files Not Found**: Verify DATASET_PATH points to your Google Drive folder\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Download the trained model from Google Drive\n",
    "2. Use the inference script locally (see cell above)\n",
    "3. Fine-tune on specific deepfake types\n",
    "4. Deploy as a web API\n",
    "5. Optimize model for mobile/edge devices"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
